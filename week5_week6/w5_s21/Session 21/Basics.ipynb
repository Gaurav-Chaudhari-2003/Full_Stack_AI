{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:35:32.592428Z",
     "start_time": "2026-01-16T06:35:30.965679300Z"
    }
   },
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\chaud\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:35:34.375997900Z",
     "start_time": "2026-01-16T06:35:34.355377600Z"
    }
   },
   "source": "from nltk.tokenize import word_tokenize, sent_tokenize",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:35:35.684817300Z",
     "start_time": "2026-01-16T06:35:35.663276300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a sample text for processing\n",
    "a = \"Hello and welcome friends to NLP workshop. My name is shridhar mankar. I will be teaching you NLP from scratch\""
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:35:37.101685300Z",
     "start_time": "2026-01-16T06:35:37.027567100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenize the text into words\n",
    "A = word_tokenize(a)\n",
    "A"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'and',\n",
       " 'welcome',\n",
       " 'friends',\n",
       " 'to',\n",
       " 'NLP',\n",
       " 'workshop',\n",
       " '.',\n",
       " 'My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'shridhar',\n",
       " 'mankar',\n",
       " '.',\n",
       " 'I',\n",
       " 'will',\n",
       " 'be',\n",
       " 'teaching',\n",
       " 'you',\n",
       " 'NLP',\n",
       " 'from',\n",
       " 'scratch']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:35:38.985435Z",
     "start_time": "2026-01-16T06:35:38.954223700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenize the text into sentences\n",
    "S = sent_tokenize(a)\n",
    "S"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello and welcome friends to NLP workshop.',\n",
       " 'My name is shridhar mankar.',\n",
       " 'I will be teaching you NLP from scratch']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Type, Length and Frequency Checking"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:36:02.088202200Z",
     "start_time": "2026-01-16T06:36:02.054395700Z"
    }
   },
   "cell_type": "code",
   "source": "type(A),len(A),type(S),len(S)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 22, list, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:36:03.189095100Z",
     "start_time": "2026-01-16T06:36:03.158521500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import Frequency Distribution class\n",
    "from nltk.probability import FreqDist\n",
    "frequency = FreqDist(A)\n",
    "frequency"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'NLP': 2, '.': 2, 'Hello': 1, 'and': 1, 'welcome': 1, 'friends': 1, 'to': 1, 'workshop': 1, 'My': 1, 'name': 1, ...})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Stemming"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:36:11.982188800Z",
     "start_time": "2026-01-16T06:36:11.958633700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import PorterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "pst = PorterStemmer()"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:36:45.899465300Z",
     "start_time": "2026-01-16T06:36:45.869493400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test stemming from a specific word\n",
    "pst.stem('Making')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'make'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:36:47.219245900Z",
     "start_time": "2026-01-16T06:36:47.190699700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply stemming to all words in the tokenized list\n",
    "for i in A:\n",
    "    print(pst.stem(i))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "and\n",
      "welcom\n",
      "friend\n",
      "to\n",
      "nlp\n",
      "workshop\n",
      ".\n",
      "my\n",
      "name\n",
      "is\n",
      "shridhar\n",
      "mankar\n",
      ".\n",
      "i\n",
      "will\n",
      "be\n",
      "teach\n",
      "you\n",
      "nlp\n",
      "from\n",
      "scratch\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:36:49.334904500Z",
     "start_time": "2026-01-16T06:36:49.286934700Z"
    }
   },
   "cell_type": "code",
   "source": "pst.stem('universal')",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'univers'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:36:50.089809200Z",
     "start_time": "2026-01-16T06:36:50.056290600Z"
    }
   },
   "cell_type": "code",
   "source": "pst.stem('universe')",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'univers'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:36:51.254820100Z",
     "start_time": "2026-01-16T06:36:51.225894600Z"
    }
   },
   "cell_type": "code",
   "source": "pst.stem('university')",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'univers'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:36:52.309281Z",
     "start_time": "2026-01-16T06:36:52.277414900Z"
    }
   },
   "cell_type": "code",
   "source": "pst.stem('alumni')",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alumni'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:36:55.536742Z",
     "start_time": "2026-01-16T06:36:55.505735100Z"
    }
   },
   "cell_type": "code",
   "source": "pst.stem('alumnus')",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alumnu'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Lemmatization"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:37:03.924066200Z",
     "start_time": "2026-01-16T06:37:03.854639600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "# Download WordNet resource\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\chaud\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:37:07.258652500Z",
     "start_time": "2026-01-16T06:37:07.245114500Z"
    }
   },
   "cell_type": "code",
   "source": "lemmatizer = WordNetLemmatizer()",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:37:08.923411700Z",
     "start_time": "2026-01-16T06:37:08.890654800Z"
    }
   },
   "cell_type": "code",
   "source": "pst.stem('trouble')",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'troubl'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:37:21.845324500Z",
     "start_time": "2026-01-16T06:37:20.421561700Z"
    }
   },
   "cell_type": "code",
   "source": "lemmatizer.lemmatize('trouble')",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trouble'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:37:23.429254300Z",
     "start_time": "2026-01-16T06:37:23.398773300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply lemmatization to the list\n",
    "for i in A:\n",
    "    print(lemmatizer.lemmatize(i))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "and\n",
      "welcome\n",
      "friend\n",
      "to\n",
      "NLP\n",
      "workshop\n",
      ".\n",
      "My\n",
      "name\n",
      "is\n",
      "shridhar\n",
      "mankar\n",
      ".\n",
      "I\n",
      "will\n",
      "be\n",
      "teaching\n",
      "you\n",
      "NLP\n",
      "from\n",
      "scratch\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:37:25.228530600Z",
     "start_time": "2026-01-16T06:37:25.197371800Z"
    }
   },
   "cell_type": "code",
   "source": "lemmatizer.lemmatize('alumnus')",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alumnus'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:37:26.701862700Z",
     "start_time": "2026-01-16T06:37:26.673208600Z"
    }
   },
   "cell_type": "code",
   "source": "lemmatizer.lemmatize('alumni')",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alumnus'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:37:28.591464600Z",
     "start_time": "2026-01-16T06:37:28.561885400Z"
    }
   },
   "cell_type": "code",
   "source": "lemmatizer.lemmatize('universe')",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'universe'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:37:29.578384Z",
     "start_time": "2026-01-16T06:37:29.548935600Z"
    }
   },
   "cell_type": "code",
   "source": "lemmatizer.lemmatize('university')",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'university'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# pos_tag"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:37:32.044411Z",
     "start_time": "2026-01-16T06:37:31.894296900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download POS tagger resource\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\chaud\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\chaud\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:37:33.836692600Z",
     "start_time": "2026-01-16T06:37:33.701447600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print POS tags for each word (Note: Tagging individually loses context)\n",
    "for i in A:\n",
    " print(nltk.pos_tag([i]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NN')]\n",
      "[('and', 'CC')]\n",
      "[('welcome', 'NN')]\n",
      "[('friends', 'NNS')]\n",
      "[('to', 'TO')]\n",
      "[('NLP', 'NN')]\n",
      "[('workshop', 'NN')]\n",
      "[('.', '.')]\n",
      "[('My', 'PRP$')]\n",
      "[('name', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('shridhar', 'NN')]\n",
      "[('mankar', 'NN')]\n",
      "[('.', '.')]\n",
      "[('I', 'PRP')]\n",
      "[('will', 'MD')]\n",
      "[('be', 'VB')]\n",
      "[('teaching', 'VBG')]\n",
      "[('you', 'PRP')]\n",
      "[('NLP', 'NN')]\n",
      "[('from', 'IN')]\n",
      "[('scratch', 'NN')]\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    •NN: Noun, singular or mass\n",
    "    •CC: Coordinating conjunction\n",
    "    •NNS: Noun, plural\n",
    "    •TO: \"to\" as preposition or infinitive marker\n",
    "    •PRP$: Possessive pronoun (e.g., my, his)\n",
    "    •VBZ: Verb, 3rd person singular present\n",
    "    •PRP: Personal pronoun (e.g., I, he, she)\n",
    "    •MD: Modal (e.g., will, can)\n",
    "    •VB: Verb, base form\n",
    "    •VBG: Verb, gerund or present participle\n",
    "    •IN: Preposition or subordinating conjunction\n",
    "    •.: Punctuation (Sentence terminator)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Named entity recognition\n",
    "\n",
    "    Identifies real-world objects (Entities).\n",
    "    Gives Labels like PERSON, GPE (Location), ORGANIZATION.\n",
    "    Focus on Semantics (Meaning/Information extraction).\n",
    "    Usually happens after POS tagging (it uses POS tags to help identify entities)."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:37:47.864466700Z",
     "start_time": "2026-01-16T06:37:47.845956700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:37:49.172224700Z",
     "start_time": "2026-01-16T06:37:49.151226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sample text for NER\n",
    "text= '''Harry Lives in New York'''\n",
    "words= word_tokenize(text)\n",
    "postags=pos_tag(words)"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:37:50.418974500Z",
     "start_time": "2026-01-16T06:37:50.322305600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\chaud\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\chaud\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:37:53.842551500Z",
     "start_time": "2026-01-16T06:37:53.628512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Perform Named Entity Chunking\n",
    "tree = nltk.ne_chunk(postags)\n",
    "print(tree)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (PERSON Harry/NNP) Lives/VBZ in/IN (GPE New/NNP York/NNP))\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:37:55.667115700Z",
     "start_time": "2026-01-16T06:37:55.645859900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Another example for NER\n",
    "text= 'John wants a new Samsung device from Pune'\n",
    "words= word_tokenize(text)\n",
    "postags=pos_tag(words)"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:38:02.207452200Z",
     "start_time": "2026-01-16T06:38:01.995997900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tree = nltk.ne_chunk(postags)\n",
    "print(tree)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON John/NNP)\n",
      "  wants/VBZ\n",
      "  a/DT\n",
      "  new/JJ\n",
      "  (ORGANIZATION Samsung/NNP)\n",
      "  device/NN\n",
      "  from/IN\n",
      "  (GPE Pune/NNP))\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Stopwords     ...which connects sentences."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:38:24.674215900Z",
     "start_time": "2026-01-16T06:38:24.654710200Z"
    }
   },
   "cell_type": "code",
   "source": "from nltk.corpus import stopwords",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:38:47.197386Z",
     "start_time": "2026-01-16T06:38:47.164633500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set of English stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chaud\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:38:53.268777800Z",
     "start_time": "2026-01-16T06:38:53.236484200Z"
    }
   },
   "cell_type": "code",
   "source": "stop_words",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " \"he's\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:43:55.879570200Z",
     "start_time": "2026-01-16T06:43:55.837273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "msg = \"My name is shridhar mankar, I love making videos and watching kdrama. My speciality is making things easy\"\n",
    "\n",
    "# Tokenize the message\n",
    "words = word_tokenize(msg)\n",
    "\n",
    "filtered_sentence = []\n",
    "\n",
    "# Filter out stopwords\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(words)\n",
    "print(filtered_sentence)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'shridhar', 'mankar', ',', 'I', 'love', 'making', 'videos', 'and', 'watching', 'kdrama', '.', 'My', 'speciality', 'is', 'making', 'things', 'easy']\n",
      "['My', 'name', 'shridhar', 'mankar', ',', 'I', 'love', 'making', 'videos', 'watching', 'kdrama', '.', 'My', 'speciality', 'making', 'things', 'easy']\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T06:44:29.150037Z",
     "start_time": "2026-01-16T06:44:29.106101900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "msg = \"My name my is shridhar mankar, I love making videos and watching kdrama. My speciality is making things easy\"\n",
    "\n",
    "# Tokenize the message\n",
    "words = word_tokenize(msg)\n",
    "\n",
    "filtered_sentence = []\n",
    "\n",
    "# Filter out stopwords\n",
    "filtered_sentence = [w for w in words if w not in stop_words]\n",
    "\n",
    "print(words)\n",
    "print(filtered_sentence)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'my', 'is', 'shridhar', 'mankar', ',', 'I', 'love', 'making', 'videos', 'and', 'watching', 'kdrama', '.', 'My', 'speciality', 'is', 'making', 'things', 'easy']\n",
      "['My', 'name', 'shridhar', 'mankar', ',', 'I', 'love', 'making', 'videos', 'watching', 'kdrama', '.', 'My', 'speciality', 'making', 'things', 'easy']\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
